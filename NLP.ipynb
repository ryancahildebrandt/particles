{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Doc setup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob, re, string\n",
    "from readin import raw_text\n",
    "import spacy, time, pickle, ginza, ja_ginza, ja_ginza_dict\n",
    "from itertools import chain\n",
    "import spacy, time, pickle, ginza, ja_ginza, ja_ginza_dict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sidetable as stb\n",
    "from itertools import chain\n",
    "nlp = spacy.load(\"ja_ginza\")\n",
    "docs = pickle.load(open(\"docs.p\", \"rb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Read in single speaker\n",
    "jsss = pd.read_csv(\"data/japanese-single-speaker-speech-dataset/transcript.txt\",\n",
    "                     header=None,\n",
    "                     sep=\"|\",\n",
    "                     names=[\"file\",\"JA\",\"RJ\",\"duration\"]).JA\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "# %% Read in subtitle corpus\n",
    "subtitles = pd.read_csv(\"data/japaneseenglish-subtitle-corpus/raw.txt\",\n",
    "                        header=None,\n",
    "                        sep=\"\\t\",\n",
    "                        names=[\"EN\",\"JA\"]).JA\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Read in knb\n",
    "knb_gourmet = pd.read_csv(\"data/knb-corpus/knbc/knbc/corpus2/Gourmet.tsv\",\n",
    "                        header=None,\n",
    "                        sep=\"\\t\",\n",
    "                        names=[\"ID\",\"JA1\",\"JA2\",\"JA3\",\"JA4\",\"JA5\"])\n",
    "knb_keitai = pd.read_csv(\"data/knb-corpus/knbc/knbc/corpus2/Keitai.tsv\",\n",
    "                        header=None,\n",
    "                        sep=\"\\t\",\n",
    "                        names=[\"ID\",\"JA1\",\"JA2\",\"JA3\",\"JA4\",\"JA5\"])\n",
    "knb_kyoto = pd.read_csv(\"data/knb-corpus/knbc/knbc/corpus2/Kyoto.tsv\",\n",
    "                        header=None,\n",
    "                        sep=\"\\t\",\n",
    "                        names=[\"ID\",\"JA1\",\"JA2\",\"JA3\",\"JA4\",\"JA5\"])\n",
    "knb_sports = pd.read_csv(\"data/knb-corpus/knbc/knbc/corpus2/Sports.tsv\",\n",
    "                        header=None,\n",
    "                        sep=\"\\t\",\n",
    "                        names=[\"ID\",\"JA1\",\"JA2\",\"JA3\",\"JA4\",\"JA5\"])\n",
    "\n",
    "knb_full = knb_gourmet.append(knb_keitai).append(knb_kyoto).append(knb_sports).fillna(\" \")\n",
    "knb_full[\"JA\"] = knb_full.JA1 + knb_full.JA2 + knb_full.JA3 + knb_full.JA4 + knb_full.JA5\n",
    "knb = knb_full.JA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Read in tatoeba\n",
    "tatoeba = pd.read_csv(\"data/tatoeba.txt\",\n",
    "                      header=None,\n",
    "                      sep=\";;\",\n",
    "                      names=[\"EN\", \"JP\", \"Source\"]).JP\n",
    "\n",
    "# %% All together now\n",
    "raw_text = jsss.append(knb).append(tatoeba).astype(\"unicode\").str.strip()#.append(subtitles)\n",
    "raw_str = raw_text.str.cat(sep=\" \").strip()\n",
    "raw_str = re.sub(r\"[^一-龯ぁ-ゞァ-ヶ０-９。、？！]\",\"\", raw_str)\n",
    "raw_str = re.sub(r\"[。]\",\"。;\", raw_str)\n",
    "#print(raw_str[:1000])\n",
    "raw_text = raw_str.split(sep=\";\")\n",
    "#print(raw_text[:100])\n",
    "print(raw_str,  file = open(\"raw_str.txt\", 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Model & Docs Spec\n",
    "start = time.time()\n",
    "\n",
    "nlp = spacy.load(\"ja_ginza\")\n",
    "docs= list(nlp.pipe(raw_text, batch_size=100, disable=[\"ner\",\"textcat\"]))\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "pickle.dump(docs, open(\"docs.p\", \"wb\" ))\n",
    "print(f\"NLP Runtime : {end - start} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% att & chunk dfs\n",
    "\n",
    "start = time.time()\n",
    "#docs=docs[:10]\n",
    "att_df = pd.DataFrame()\n",
    "count=0\n",
    "for doc in docs:\n",
    "    data = pd.DataFrame([[i.text,i.head.text,i.head.pos_,\n",
    "                        i.head.tag_,i.head.dep_,i.lemma_,i.norm_,\n",
    "                        i.is_stop,i.pos_,i.tag_,i.dep_,\n",
    "                        [child.text for child in i.children]] for i in doc])\n",
    "    att_df=att_df.append(data)\n",
    "    count +=1\n",
    "    print(str((len(docs)-count))+\" out of \"+str(len(docs))+\" remaining, \"+(\"{:.0%}\".format(count/len(docs)))+\" done\")\n",
    "    \n",
    "att_df.columns=[\"Token\",\"Head\",\"HeadPOS\",\"HeadTag\",\"HeadDep\",\n",
    "                      \"Lemma\",\"Norm\",\"Stop\",\"POS\",\"Tag\",\"Dep\",\"Children\"]    \n",
    "att_df.to_csv(\"att_df.csv\")\n",
    "\n",
    "end = time.time()\n",
    "print(f\"ATT Runtime : {end - start} seconds\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Spyder)",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
